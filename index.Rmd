---
title: "An Introduction to Machine Learning"
subtitle: "A Brief Overview & Demo"
author: "Joshua Rosenberg and Shaun Kellogg"
institute: "The LASER Institute"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css:
       - default
       - css/laser.css
       - css/laser-fonts.css
    lib_dir: libs
    seal: false
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: |
         <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
         </div>
---

class: clear, title-slide, inverse, center, top, middle

# `r rmarkdown::metadata$title`
## `r rmarkdown::metadata$subtitle`
### `r rmarkdown::metadata$author`
###  `r Sys.Date()`

---

class: clear, inverse, center, middle

background-image: url(https://imgs.xkcd.com/comics/tasks_2x.png)
background-position: center
background-size: contain

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

---

class: clear, inverse, center, middle

# Agenda

---

# Agenda

1. Introductions (5 min.)
1. Overview of machine learning (20 min.)
1. Machine learning demo for a _regression_ problem (15 min.)
1. Introduction to _classification_ (5 min.)
1. Discussion of extensions (5 min.)
1. Conclusion and resources to learn and do more and discussion/Q&A (10 min.)

---

class: clear, inverse, center, middle

# Introductions  

---

# Introductions

- **Introduce yourself** and your position and affiliation  
- **Describe one thing that you're not good at that you like to do**  
- **Describe one thing that you're good at that you don't like to do**  

---

# Goals of this workshop

### Over-arching goal:

*Get started with applying machine learning methods in R*  

### Specific aims are to learn about:

- a small but important set of core ideas about machine learning
- how these ideas are instantiated through the *tidymodels* R package

---

class: clear, inverse, center, middle

# Overview of Machine Learning (ML)

---

# Defining ML

- *Artificial Intelligence (AI)*
.footnote[
[1]  I feel super uncomfortable bringing AI into this, but perhaps it's useful just to be clear about terminology
]
: Simulating human intelligence through the use of computers
- *Machine learning (ML)*: A subset of AI focused on how computers acquire new information/knowledge

This definition leaves a lot of space for a range of approaches to ML

---

# One example from Josh's work

**Using Twitter users profile descriptions to predict whether or not they were teachers**

- Manually coded > 500 profiles for whether they were teachers or non-teachers
- Trained a Support Vector Machine model (or algorithm) on _text features_ using 90% of the data
- For the remaining 10% of the data, we predicted whether or not someone was a teacher with an accuracy of 85%; $\kappa$ = 0.612.
- Then used the trained classifier to predict whether or not 1000s of users were or were not teachers (and used these classifications in a multi-level model)

Rosenberg et al., *AERA Open* (2021)

https://journals.sagepub.com/doi/full/10.1177/23328584211024261

---

background-image: url("https://i.vas3k.ru/7vx.jpg")
background-position: center
background-size: contain

---

# Supervised & unsupervised

## Supervised ML

- Requires coded data or data with a known outcome
- Uses coded/outcome data to train an algorithm
- Uses that algorithm to **predict the codes/outcomes for new data** (data not used during the training)
- Can take the form of a *classification* (predicting a dichotomous or categorical outcome) or a *regression* (predicting a continuous outcome)
- Algorithms include:
  - [Linear regression (really!)](https://web.stanford.edu/~hastie/ElemStatLearn/)
  - Logistic regression
  - Decision tree
  - Support Vector Machine

---

# What kind of coded data?

> Want to detect spam? Get samples of spam messages. Want to forecast stocks? Find the price history. Want to find out user preferences? Parse their activities on Facebook (no, Mark, stop collecting it, enough!) (from [ML for Everyone](https://vas3k.com/blog/machine_learning/))

In educational research:

- Assessment data (e.g., [1](https://link.springer.com/article/10.1007/s10956-020-09895-9))
- Data from log files ("trace data") (e.g., [1](https://www.tandfonline.com/doi/full/10.1080/10508406.2013.837391?casa_token=-8Fm2KCFJ30AAAAA%3Altbc8Y8ci_z-uLJx4se9tgvru9mzm3yqCTFi12ndJ5uM6RDl5YJGG6_4KpUgIK5BYa_Ildeh2qogoQ))
- Open-ended written responses (e.g., [1](https://link.springer.com/article/10.1007/s10956-020-09889-7), [2](https://doi.org/10.1007/s11423-020-09761-w))
- Achievement data (i.e., end-of-course grades) (e.g., [1](https://link.springer.com/article/10.1007/s10956-020-09888-8), [2](https://search.proquest.com/docview/2343734516?pq-origsite=gscholar&fromopenview=true))

What else?
- Drawings/diagrammatic models
- Data collected for formative purposes (i.e., exit tickets)
- ???

---

# How is this different from regression?

The _aim_ is different, the algorithms and methods of estimation are not (or, are differences in degree, rather than in kind).

In a linear regression, our aim is to estimate parameters, such as $\beta_0$ (intercept) and $\beta_1$ (slope), and to make inferences about them that are not biased by our particular sample.

In an ML approach, we can use the same linear regression model, but with a goal other than making unbiased inferences about the $\beta$ parameters:

<h4><center>In supervised ML, our goal is to minimize the difference between a known $y$ and our predictions, $\hat{y}$</center></h3>

---

# So, how is this really different?

This _predictive goal_ means that we can do things differently:

- Multicollinearity is not an issue because we do not care to make inferences about parameters
- Because interpreting specific parameters is less of an interest, we can use a great deal more predictors
- We focus not on $R^2$ as a metric, but, instead, how accurately a _trained_ model can predict the values in _test_ data
- We can make our models very complex (but may wish to "regularize" coefficients that are small so that their values are near-zero or are zero):
  - Ridge models (can set parameters near to zero)
  - Lasso models (can set parameters to zero)
  - Elastic net models (can balance between ridge and lasso models)

---

# Okay, _really_ complex

- Okay, _really_ complex:
  - Neural networks
  - Deep networks
- And, some models can take a different form than familiar regressions:
  - *k*-nearest neighbors
  - Decision trees (and their extensions of bagged and random forests)
- Last, the modeling process can look different:
  - Ensemble models that combine or improve on ("boosting") the predictions of individual models

---

# A helpful frame 

## Unsupervised ML

- Does not require coded data; one way to think about unsupervised ML is that its purpose is to discover codes/labels
- Is used to discover groups among observations/cases or to summarize across variables
- Can be used in an _exploratory mode_ (see [Nelson, 2020](https://journals.sagepub.com/doi/full/10.1177/0049124118769114?casa_token=EV5XH31qbyAAAAAA%3AFg09JQ1XHOOzlxYT2SSJ06vZv0jG-s4Qfz8oDIQwh2jrZ-jrHNr7xZYL2FwnZtZiokhPalvV1RL2Bw)) 
- **Warning**: The results of unsupervised ML _cannot_ directly be used to provide codes/outcomes for supervised ML techniques 
- Can work with both continuous and dichotomous or categorical variables
- Algorithms include:
  - Cluster analysis
  - [Principle Components Analysis (really!)](https://web.stanford.edu/~hastie/ElemStatLearn/)
  - Latent Dirichlet Allocation (topic modeling)

---

# What technique should I choose?

Do you have coded data or data with a known outcome -- let's say about K-12 students -- and, do you want to:

- _Predict_ how other students with similar data (but without a known outcome) perform?
- _Scale_ coding that you have done for a sample of data to a larger sample?
- _Provide timely or instantaneous feedback_, like in many learning analytics systems?

<bold><h4><center>Supervised methods may be your best bet</center></h4></bold>

---

# What technique should I choose?

Do you not yet have codes/outcomes -- and do you want to?

- _Achieve a starting point_ for qualitative coding, perhaps in a ["computational grounded theory"](https://journals.sagepub.com/doi/full/10.1177/0049124117729703) mode?
- _Discover groups or patterns in your data_ that may be of interest?
- _Reduce the number of variables in your dataset_ to a smaller, but perhaps nearly as explanatory/predictive - set of variables?

<bold><h4><center>Unsupervised methods may be helpful</center></h4></bold>

---

# But, how do I select a model?

One general principle is to **start with the simplest useful model** and to _build toward
more complex models as helpfuL_.

This principle applies in multiple ways:

- To choose an algorithm, start with simpler models that you can efficiently use and understand
- To carry out feature engineering, understand your predictors well by starting with a subset
- To tune an algorithm, start with a relatively simple set of tuning parameters

This isn't just for beginners or those of us in education; [most spam filters use Support Vector Machines (and used Naive Bayes until recently)](https://vas3k.com/blog/machine_learning/) due to their combination of effectiveness and efficiency "in production."

---

# Super complex?

- Nothing too much, apart from computing power, time, and concerns of 
- A "check" on your work is your predictions on _test_ set data
  - *Train data*: Coded/outcome data that you use to train ("estimate") your model
  - *Validation data<sup>1</sup>*: Data you use to select a particular algorithm
  - *Test ("hold-out") data*: Data that you do not use in any way to train your data

- An important way to achieve good performance with test data is to balance between the inherent _bias_ in your algorithm and the _variance_ in the predictions of your algorithm; this is referred to as the **bias-variance** trade-off of _all_ models

.footnote[
[1] not always/often used, for reasons we'll discuss later

]

---

# Illustrating the bias-variance tradeoff

```{r, echo = FALSE, fig.align="center", message = FALSE}
library(ggplot2)

set.seed(5)
dat = data.frame(x = runif(1000, 0, 12))
dat$x = sort(dat$x)
dat$y = with(dat, sin(x * 1.3) * 15 + 3 * (x - 4)^2)
sigma = with(dat, (exp(x - 5)/(1 + exp(x - 5)) - exp(x - 7)/(1 + exp(x - 7)) * 
    2) + 1.4) * 6
dat$yobs = dat$y + rnorm(nrow(dat), mean = 0, sd = sigma)

ggplot(dat, aes(x = x, y = yobs)) +
  geom_point(color = "darkgray") +
  ylab("pred")

ggsave("img/bias-variance-data-1.png")
```

---

# Strong bias

```{r, echo = FALSE, fig.align="center", message = FALSE}
dat = data.frame(x = runif(1000, 0, 12))
dat$x = sort(dat$x)
dat$y = with(dat, sin(x * 1.3) * 15 + 3 * (x - 4)^2)
sigma = with(dat, (exp(x - 5)/(1 + exp(x - 5)) - exp(x - 7)/(1 + exp(x - 7)) * 
    2) + 1.4) * 6
dat$yobs = dat$y + rnorm(nrow(dat), mean = 0, sd = sigma)

ggplot(dat, aes(x = x, y = yobs)) +
  geom_point(color = "darkgray") +
  geom_smooth(method = "lm") +
  ylab("pred")

ggsave("img/bias-variance-data-3.png")
```

---

# A much less-biased algorithm

```{r, echo = FALSE, fig.align="center", message = FALSE}
set.seed(5)
dat = data.frame(x = runif(1000, 0, 12))
dat$x = sort(dat$x)
dat$y = with(dat, sin(x * 1.3) * 15 + 3 * (x - 4)^2)
sigma = with(dat, (exp(x - 5)/(1 + exp(x - 5)) - exp(x - 7)/(1 + exp(x - 7)) * 
    2) + 1.4) * 6
dat$yobs = dat$y + rnorm(nrow(dat), mean = 0, sd = sigma)

ggplot(dat, aes(x = x, y = yobs)) +
  geom_point(color = "darkgray") +
  geom_smooth(method = lm, formula = y ~ splines::bs(x, 10), se = FALSE) +
  ylab("pred")

ggsave("img/bias-variance-data-5.png")
```

---

# Slightly different data (right pane)

```{r, echo = FALSE, echo = FALSE, message = FALSE, include = FALSE}
dat = data.frame(x = runif(1000, 0, 12))
dat$x = sort(dat$x)
dat$y = with(dat, sin(x * 2.3) * 25 + 3 * (x - 3)^2)
sigma = with(dat, (exp(x - 5)/(1 + exp(x - 5)) - exp(x - 7)/(1 + exp(x - 7)) * 
    2) + 1.4) * 6
dat$yobs = dat$y + rnorm(nrow(dat), mean = 0, sd = sigma)

ggplot(dat, aes(x = x, y = yobs)) +
  geom_point(color = "darkgray") +
  ylab("pred")

ggsave("img/bias-variance-data-2.png")
```

.pull-left[
<img src="img/bias-variance-data-1.png" width=300 height=400>
]

.pull-right[
<img src="img/bias-variance-data-2.png" width=300 height=400>
]

---

# Still strong bias, but low variance

```{r, echo = FALSE, fig.align="center", message = FALSE, include = FALSE}
dat = data.frame(x = runif(1000, 0, 12))
dat$x = sort(dat$x)
dat$y = with(dat, sin(x * 2.3) * 25 + 3 * (x - 3)^2)
sigma = with(dat, (exp(x - 5)/(1 + exp(x - 5)) - exp(x - 7)/(1 + exp(x - 7)) * 
    2) + 1.4) * 6
dat$yobs = dat$y + rnorm(nrow(dat), mean = 0, sd = sigma)

ggplot(dat, aes(x = x, y = yobs)) +
  geom_point(color = "darkgray") +
  geom_smooth(method = "lm") +
  ylab("pred")

ggsave("img/bias-variance-data-4.png")
```

.pull-left[
<img src="img/bias-variance-data-3.png" width=300 height=400>
]

.pull-right[
<img src="img/bias-variance-data-4.png" width=300 height=400>
]

---

# Low bias, but very high variance

```{r, echo = FALSE, fig.align="center", message = FALSE, include = FALSE}
dat = data.frame(x = runif(1000, 0, 12))
dat$x = sort(dat$x)
dat$y = with(dat, sin(x * 2.3) * 25 + 3 * (x - 3)^2)
sigma = with(dat, (exp(x - 5)/(1 + exp(x - 5)) - exp(x - 7)/(1 + exp(x - 7)) * 
    2) + 1.4) * 6
dat$yobs = dat$y + rnorm(nrow(dat), mean = 0, sd = sigma)

ggplot(dat, aes(x = x, y = yobs)) +
  geom_point(color = "darkgray") +
  geom_smooth(method = lm, formula = y ~ splines::bs(x, 10), se = FALSE) +
  ylab("pred")

ggsave("img/bias-variance-data-6.png")
```

.pull-left[
<img src="img/bias-variance-data-5.png" width=300 height=400>
]

.pull-right[
<img src="img/bias-variance-data-6.png" width=300 height=400>
]

---

# The bias-variance tradeoff

.pull-left[

#### Bias

- *Definition*: Difference between our known codes/outcomes and our predicted codes/outcomes; difference between $y$ and $\hat{y}$

- How (in)correct our models' (algorithms') predictions are

- Models with high bias can fail to capture important relationships--they can be *under-fit* to our data

- In short, how well our model reflects the patterns in the data

]

.pull-right[

#### Variance

- *Definition*: Using a different sample of data, the difference in $\hat{y}$ values

- How sensitive our predictions are to the specific sample on which we trained the model 
- Models with high variance can fail to predict different data well--they can be *over-fit* to our data

- In short, how stable the predictions of our model are

]

<h4><center>Regardless of model, we often wish to balance between bias and variance</center></h4>

---


class: clear, inverse, center, middle

# Machine learning demo: Regression

---

# Overview of ML regression

- We are interested in predicting a _continuous_ outcome; accordingly, our predictions are _continuous_
- Metrics for how well the model performed include:
  - Root Mean Square Error (RMSE): can be interpreted as the _average_ difference between $y$ and $\hat{y}$
  - Mean Absolute Error (MAE): can be interpreted as the _median_ difference between $y$ and $\hat{y}$
  - $R^2$: Proportion of variance explained (used descriptively)

---

# Overview of regression modeling

1. Split data (into train, test, and tuning sets sets)
1. Specify recipe, model, and workflow
1. Estimate models and determine tuning parameters' values
1. Evaluate metrics and predictions

---

# Data from online science classes

- This data comes from a study of ~700 secondary/high school-aged students in the United States
- These were "one-off" classes that helped to fill gaps in students' schedules
- The data were collected _over multiple semesters_ from _multiple classes_
- There are a number of types of variables:
  - Demographic/contextual variables, e.g. `subject` and `gender`
  - Self-report survey variables: `uv` (utility value), `percomp` (perceived competence), and `int` (interest)
  - Gradebook variables: `percentage_earned` (and others)
  - Discussion variables: `sum_discussion_posts`, `sum_n_words`
  - Outcomes: `final_grade`, `passing_grade`, `time_spent` (in minutes)

---

# Let's look at these steps once more

1. Split data (into train, test, and tuning sets sets)
1. Specify recipe, model, and workflow
1. Estimate models and determine tuning parameters' values
1. Evaluate metrics and predictions

We'll run these in `ml-regression-demo.Rmd` in [RStudio Cloud](https://rstudio.cloud/spaces/140883/project/3890610)!

---

# Demo (RStudio Cloud)

```{r xaringan-panelset, echo=FALSE}
xaringanExtra::use_panelset()
```

.panelset[
.panel[.panel-name[1]

**Split data (into train, test, and tuning sets sets)**

```{r panel-chunk-1, fig.show='hide', eval = FALSE}
train_test_split <- initial_split(d, prop = .70)

data_train <- training(train_test_split)

kfcv <- vfold_cv(data_train)
```
]

.panel[.panel-name[2]

**Specify recipe, model, and workflow**
 
```{r panel-chunk-2, fig.show='hide', eval = FALSE}
sci_rec <- recipe(final_grade ~ ., data = d) %>% 
    ...

rf_mod_many <-rand_forest(mtry = tune(),
              min_n = tune()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

rf_wf_many <- workflow() %>%
  add_model(rf_mod_many) %>% 
  add_recipe(sci_rec)
```
]

.panel[.panel-name[3]

**Estimate models and determine tuning parameters' values**

```{r panel-chunk-4, fig.show='hide', eval = FALSE}
tree_res <- rf_wf_many %>% 
  tune_grid(
    resamples = kfcv,
    grid = tree_grid,
    metrics = metric_set(rmse, mae, rsq)
  )

# select best set of tuning parameters
best_tree <- tree_res %>%
  select_best()

# finalize workflow with best set of tuning parameters
final_wf <- rf_wf_many %>% 
  finalize_workflow(best_tree)
```
]

.panel[.panel-name[4]

**Evaluate metrics and predictions**

```{r panel-chunk-5, fig.show='hide', eval = FALSE}
# fit split data (separately)
final_fit <- final_wf %>% 
  last_fit(train_test_split, metrics = metric_set(rmse, mae, rsq))

# fit stats
final_fit %>%
  collect_metrics()
```
]
]

---

# Demo

Run these steps in `ml-regression-demo.Rmd` in [RStudio Cloud](https://rstudio.cloud/spaces/140883/project/3890610).

Prompts for discussion: 

- What do you take away from this analysis overall?
- What changes could we make to improve the model?

---

class: clear, inverse, center, middle

# Intro. to classification

---

# Overview of classification

- We are interested in predicting a _dichotomous_ or _continuous_ outcome; our predictions are _probabilities_
- These probabilities are often changed (through the use of a threshold - often 0.50) back into dichotomous or categorical labels
- Metrics for how well the model performed include:
  - *Accuracy*: For the known codes, what percentage of the predictions are correct
  - *Cohen's $\kappa$*: Same as accuracy, while account for the base rate of (chance) agreement
  - *Sensitivity (AKA recall)*: Among the true "positives", what percentage are classified as "positive"?
  - *Specificity*: Among the true "negatives", what percentage are classified as "negative"?
  - *ROC AUC*: For different levels of the threshold, what is the sensitivity and specificity?

---

# Overview of classification modeling

*Nearly identical to that for regression*

1. Split our data into train and test sets
  - Split train set further into *k*-folds to use for determining tuning parameters (validation)
1. Carry out feature engineering and specify our formula
1. Estimate our model and determine tuning parameters
1. Using final model from the training data set, predict the $\hat{y}$ and compare to our known $y$ for our test data and then calculate metrics (Accuracy, $\kappa$, Sensitivity, Specificity, ROC (plot), AUC value)

---
class: clear, inverse, center, middle

# Discussion of Extensions

---

# Text analysis (NLP)

```{r, echo = FALSE, out.width="70%"}
knitr::include_graphics("img/dtm.png")
```

Generality classifier: https://faast.shinyapps.io/generality-shiny/

Other extensions: Audio and video data and xxx?

---

class: clear, inverse, center, middle

# Conclusion  

---

# Learning more

- [tidymodels](https://www.tidymodels.org/)
- [Hands-on Machine Learning With R](https://bradleyboehmke.github.io/HOML/)
- [Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/)
- [Learning to Teach Machines to Learn](https://alison.rbind.io/post/2019-12-23-learning-to-teach-machines-to-learn/)
- [Julia Silge's blog](https://juliasilge.com/blog/)

---

# Recommendations

- Start with a problem you are facing
- Use a simple algorithm that you can understand and troubleshoot
- Be mindful of your R code; small issues (with names!) can cause difficulties
- Share and receive feedback
- Explore the variety of ways you can use machine learning; we are deciding as a field how machine learning will (or will not) make a difference in our work
- It _will be really hard at first_; you can do this!

---

# Thanks!

Joshua Rosenberg  
jmrosenberg@utk.edu  
@jrosenberg6432    

Shaun Kellogg  
sbkellog@ncsu.edu  
@sbkellogg  

Slides created via the R package [xaringan](https://github.com/yihui/xaringan)

Available: 
